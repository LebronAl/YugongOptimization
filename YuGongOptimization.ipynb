{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## load data\n",
    "cluster_capacity_df = pd.read_csv('./data/tmp_cluster_20201029', names=['cluster_name', 'cluster_total_cpu', 'cluster_total_storage']).drop_duplicates()\n",
    "table_cpu_cost_df = pd.read_csv('./data/tmp_table_cpu_cost_20201029', names=['project_name','table_name','job_count', 'table_cost_cpu']).drop_duplicates()\n",
    "table_job_df = pd.read_csv('./data/tmp_table_input_relevence_20201029', names=['ndTable', 'stTable', 'relevence_size']).drop_duplicates()\n",
    "table_updown_df = pd.read_csv('./data/tmp_table_output_relevence_20201029', names=['output', 'input', 'relevence_size']).drop_duplicates()\n",
    "current_placement_df = pd.read_csv('./data/current_project_location', names=['project_name', 'cluster_name']).drop_duplicates()\n",
    "greed_placement_np = np.load('./data/cluster_key_copy02.npy', allow_pickle=True).tolist()\n",
    "\n",
    "## clean data\n",
    "cluster_capacity_df['cluster_name'] = cluster_capacity_df['cluster_name'].str.lower()\n",
    "cluster_capacity_df.fillna(value={'cluster_total_cpu':0,'cluster_total_storage':0}, inplace=True)\n",
    "\n",
    "table_cpu_cost_df['project_name'] = table_cpu_cost_df['project_name'].str.lower()\n",
    "table_cpu_cost_df['table_name'] = table_cpu_cost_df['table_name'].str.lower()\n",
    "table_cpu_cost_df.fillna(value={'table_cost_cpu':0}, inplace=True)\n",
    "\n",
    "table_job_df['ndTable'] = table_job_df['ndTable'].str.lower()\n",
    "table_job_df['stTable'] = table_job_df['stTable'].str.lower()\n",
    "table_job_df.fillna(value={'relevence_size':0}, inplace=True)\n",
    "\n",
    "table_updown_df['output'] = table_updown_df['output'].str.lower()\n",
    "table_updown_df['input'] = table_updown_df['input'].str.lower()\n",
    "table_updown_df.fillna(value={'relevence_size':0}, inplace=True)\n",
    "\n",
    "current_placement_df['project_name'] = current_placement_df['project_name'].str.lower()\n",
    "current_placement_df['cluster_name'] = current_placement_df['cluster_name'].str.lower()\n",
    "\n",
    "\n",
    "## data info\n",
    "print(\"--------------------\")\n",
    "print(\"ClusterCapacityInfo : size = {}\".format(\n",
    "    cluster_capacity_df.shape[0]))\n",
    "print(cluster_capacity_df.dtypes)\n",
    "print(cluster_capacity_df.head())\n",
    "print(\"--------------------\")\n",
    "print(\"TableCpuInfo : size = {}\".format(table_cpu_cost_df.shape[0]))\n",
    "print(table_cpu_cost_df.dtypes)\n",
    "print(table_cpu_cost_df.head())\n",
    "print(\"--------------------\")\n",
    "print(\"TableJobInfo : size = {}\".format(\n",
    "    table_job_df.shape[0]))\n",
    "print(table_job_df.dtypes)\n",
    "print(table_job_df.head())\n",
    "print(\"--------------------\")\n",
    "print(\"TableUpDownInfo : size = {}\".format(\n",
    "            table_updown_df.shape[0]))\n",
    "print(table_updown_df.dtypes)\n",
    "print(table_updown_df.head())\n",
    "print(\"--------------------\")\n",
    "print(\"CurrentPlacementInfo : size = {}\".format(\n",
    "            current_placement_df.shape[0]))\n",
    "print(current_placement_df.dtypes)\n",
    "print(current_placement_df.head())\n",
    "print(\"--------------------\")\n",
    "print(\"GreedyPlacementInfo : size = {}\".format(\n",
    "            len(greed_placement_np)))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cluster_size = cluster_capacity_df.shape[0]\n",
    "table_size = table_cpu_cost_df.shape[0]\n",
    "table_job_size = table_job_df.shape[0]\n",
    "table_updown_size = table_updown_df.shape[0]\n",
    "current_placement_size = current_placement_df.shape[0]\n",
    "\n",
    "## get cluster Probability during choosing cluster based on this total cpu\n",
    "clusterRouletteProbability = cluster_capacity_df['cluster_total_cpu'] / cluster_capacity_df['cluster_total_cpu'].sum()\n",
    "print('----clusterRouletteProbability----')\n",
    "print(clusterRouletteProbability)\n",
    "print('--------')\n",
    "\n",
    "## inverted index from cluster to their index\n",
    "cluster2index = {}\n",
    "for index in tqdm(range(cluster_size), desc='Processing'):\n",
    "    cluster2index[cluster_capacity_df.iloc[index, 0]] = index\n",
    "\n",
    "## inverted index from table to their index\n",
    "table2index = {}\n",
    "for index in tqdm(range(table_size), desc='Processing'):\n",
    "    table2index[table_cpu_cost_df.iloc[index, 0] + '.' + table_cpu_cost_df.iloc[index, 1]] = index\n",
    "    \n",
    "## inverted index from project to their cluster index\n",
    "project2clusterindex = {}\n",
    "for index in tqdm(range(current_placement_size), desc='Processing'):\n",
    "    try:\n",
    "        project2clusterindex[current_placement_df.iloc[index,0]] = cluster2index[current_placement_df.iloc[index,1]]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ## get total Flow\n",
    "# totalFlow = 0\n",
    "# for index in tqdm(range(table_job_size), desc='Processing'):\n",
    "#     totalFlow += table_job_df.iloc[index, 2]    \n",
    "# for index in tqdm(range(table_updown_size), desc='Processing'):\n",
    "#     totalFlow += table_updown_df.iloc[index, 2]\n",
    "\n",
    "\n",
    "##----totalFlow----\n",
    "##305053213.15901905\n",
    "##--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "## evalute roulette Random algorithm\n",
    "def rouletteRandom(probability):\n",
    "    sum = 0\n",
    "    ran = random.random()\n",
    "    for num, r in zip(range(len(probability)), probability):\n",
    "        sum += r\n",
    "        if ran < sum :\n",
    "            break\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each individual, we calculate it's assessment, which is innerFlow and innerFlow.\n",
    "## Addition, we stores the usedCPUMap(key=cluster_index,value=totalCPUForThisPlancement) and clusterTableMap(key=cluster_index,value=list(table_index))\n",
    "class Individual:\n",
    "    \n",
    "    def __init__(self, entity, usedCPUMap, clusterTableMap, execute):\n",
    "        self.entity = entity\n",
    "        self.innerFlow = 0;\n",
    "        self.crossFlow = 0\n",
    "        self.usedCPUMap = usedCPUMap\n",
    "        self.clusterTableMap = clusterTableMap\n",
    "        if execute:\n",
    "            self.figureAssessment()\n",
    "        \n",
    "    def figureAssessment(self):\n",
    "        self.figureTableJob()\n",
    "        self.figureTableUpdown()\n",
    "        self.printInfo()\n",
    "        \n",
    "    def printInfo(self):\n",
    "        print(\"----IndividualInfo----\")\n",
    "        print(\"InnerFlow: \" + str(self.innerFlow))\n",
    "        print(\"CrossFlow: \" + str(self.crossFlow))\n",
    "        print(\"CPUUsage: \")\n",
    "        for index in range(len(self.usedCPUMap)):\n",
    "            print(str(index) + \":\" + str(self.usedCPUMap[index] / cluster_capacity_df.iloc[index, 1]))\n",
    "        print(\"--------\")\n",
    "        \n",
    "    def figureTableJob(self):\n",
    "        for index in range(table_job_size):\n",
    "            if table_job_df.iloc[index, 0] in table2index.keys():\n",
    "                left = table2index[table_job_df.iloc[index, 0]]\n",
    "            else:\n",
    "                continue\n",
    "            if table_job_df.iloc[index, 1] in table2index.keys():\n",
    "                right = table2index[table_job_df.iloc[index, 1]]\n",
    "            else:\n",
    "                continue\n",
    "            if self.entity[left] != self.entity[right]:\n",
    "                self.crossFlow += table_job_df.iloc[index, 2]\n",
    "            else:\n",
    "                self.innerFlow += table_job_df.iloc[index, 2]\n",
    "        \n",
    "    def figureTableUpdown(self):\n",
    "        for index in range(table_updown_size):\n",
    "            if table_updown_df.iloc[index, 0] in table2index.keys():\n",
    "                left = table2index[table_updown_df.iloc[index, 0]]\n",
    "            else:\n",
    "                continue\n",
    "            if table_updown_df.iloc[index, 1] in table2index.keys():\n",
    "                right = table2index[table_updown_df.iloc[index, 1]]\n",
    "            else:\n",
    "                continue\n",
    "            if self.entity[left] != self.entity[right]:\n",
    "                self.crossFlow += table_updown_df.iloc[index, 2]\n",
    "            else:\n",
    "                self.innerFlow += table_updown_df.iloc[index, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random generates a valid ndividual along with it's usedCPUMap and clusterTableMap\n",
    "def figureInitPopulation():\n",
    "    usedCPUMap = np.zeros([cluster_size], dtype = np.int64)\n",
    "    clusterTableMap = {}\n",
    "    entity = np.zeros([table_size], dtype = np.int8)\n",
    "    for index in tqdm(range(table_size), desc='Processing'):\n",
    "        while True:\n",
    "            choosedCluster = rouletteRandom(clusterRouletteProbability)\n",
    "            if usedCPUMap[choosedCluster] + table_cpu_cost_df.iloc[index, 3] <= cluster_capacity_df.iloc[choosedCluster, 1]:\n",
    "                usedCPUMap[choosedCluster] += table_cpu_cost_df.iloc[index, 3]\n",
    "                if choosedCluster not in clusterTableMap.keys():\n",
    "                    clusterTableMap[choosedCluster] = []\n",
    "                clusterTableMap[choosedCluster].append(index)\n",
    "                entity[index] = choosedCluster\n",
    "                break\n",
    "    return Individual(entity, usedCPUMap, clusterTableMap, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figureCurrentPlacement():\n",
    "    usedCPUMap = np.zeros([cluster_size], dtype = np.int64)\n",
    "    clusterTableMap = {}\n",
    "    entity = np.zeros([table_size], dtype = np.int8)\n",
    "    for index in tqdm(range(table_size), desc='Processing'):\n",
    "        try: \n",
    "            choosedCluster = project2clusterindex[table_cpu_cost_df.iloc[index, 0]]\n",
    "        except:\n",
    "            choosedCluster = rouletteRandom(clusterRouletteProbability)\n",
    "        usedCPUMap[choosedCluster] += table_cpu_cost_df.iloc[index, 3]\n",
    "        if choosedCluster not in clusterTableMap.keys():\n",
    "            clusterTableMap[choosedCluster] = []\n",
    "        clusterTableMap[choosedCluster].append(index)\n",
    "        entity[index] = choosedCluster\n",
    "    return Individual(entity, usedCPUMap, clusterTableMap, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figureGreedyPlacement():\n",
    "    usedCPUMap = np.zeros([cluster_size], dtype = np.int64)\n",
    "    clusterTableMap = {}\n",
    "    entity = np.zeros([table_size], dtype = np.int8)\n",
    "    for index in tqdm(range(table_size), desc='Processing'):\n",
    "        choosedCluster = cluster2index[greed_placement_np[table_cpu_cost_df.iloc[index, 0] + '.' + table_cpu_cost_df.iloc[index, 1]].lower()]\n",
    "        usedCPUMap[choosedCluster] += table_cpu_cost_df.iloc[index, 3]\n",
    "        if choosedCluster not in clusterTableMap.keys():\n",
    "            clusterTableMap[choosedCluster] = []\n",
    "        clusterTableMap[choosedCluster].append(index)\n",
    "        entity[index] = choosedCluster\n",
    "    return Individual(entity, usedCPUMap, clusterTableMap, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init cpu_num - 1 individuals for first population using alomost all cpu process to accelerate \n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "population = []\n",
    "population_size = multiprocessing.cpu_count() - 1\n",
    "pool = multiprocessing.Pool(population_size)\n",
    "\n",
    "results = []\n",
    "# results.append(pool.apply_async(func=figureGreedyPlacement))\n",
    "results.append(pool.apply_async(func=figureCurrentPlacement))\n",
    "\n",
    "for i in range(population_size - 1):\n",
    "    results.append(pool.apply_async(func=figureInitPopulation))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "for result in results:\n",
    "    population.append(result.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select two individuals for population based on their assessment and probability\n",
    "def selection(population):\n",
    "    sum = 0\n",
    "    populationProbability = []\n",
    "    for individual in population:\n",
    "        sum += individual.innerFlow\n",
    "    for individual in population:\n",
    "        populationProbability.append(individual.innerFlow / sum)\n",
    "    individual_1 = rouletteRandom(populationProbability)\n",
    "    individual_2 = rouletteRandom(populationProbability)\n",
    "    while individual_1 == individual_2:\n",
    "            individual_2 = rouletteRandom(populationProbability)\n",
    "    return individual_1, individual_2\n",
    "\n",
    "## generate one individuals base on their parents to store good gene\n",
    "def crossover(individual_1, individual_2):\n",
    "    stage = 1000\n",
    "    entity = np.zeros([table_size], dtype = np.int8)\n",
    "    usedCPUMap = np.zeros([cluster_size], dtype = np.int64)\n",
    "    clusterTableMap = {}\n",
    "    for index in range(table_size):\n",
    "        if index % stage == 0:\n",
    "            if index + stage <= table_size:\n",
    "                end = index + stage\n",
    "            else:\n",
    "                end = table_size\n",
    "            if random.randint(0,1) == 0:\n",
    "                for i in range(index, end):\n",
    "                    entity[i] = population[individual_1].entity[i]\n",
    "            else:\n",
    "                for i in range(index, end):\n",
    "                    entity[i] = population[individual_2].entity[i]\n",
    "        usedCPUMap[entity[index]] += table_cpu_cost_df.iloc[index, 3]\n",
    "        if entity[index] not in clusterTableMap.keys():\n",
    "            clusterTableMap[entity[index]] = []\n",
    "        clusterTableMap[entity[index]].append(index)\n",
    "    return Individual(entity, usedCPUMap, clusterTableMap, False)\n",
    "\n",
    "## mutate this individuals to extent search space\n",
    "def mutate(individual):\n",
    "    mutateProbability = 0.01\n",
    "    num = int(table_size * mutateProbability)\n",
    "    for _ in range(num):\n",
    "        chooseTable = random.randint(0, table_size - 1)\n",
    "        newCluster = rouletteRandom(clusterRouletteProbability)\n",
    "        if newCluster != individual.entity[chooseTable]:\n",
    "            individual.usedCPUMap[individual.entity[chooseTable]] -= table_cpu_cost_df.iloc[chooseTable, 3]\n",
    "            individual.usedCPUMap[newCluster] += table_cpu_cost_df.iloc[chooseTable, 3]\n",
    "            individual.clusterTableMap[individual.entity[chooseTable]].remove(chooseTable)\n",
    "            individual.clusterTableMap[newCluster].append(chooseTable)\n",
    "            individual.entity[chooseTable] = newCluster\n",
    "            \n",
    "\n",
    "## repair this individual to make it feasible after crossover and mutate\n",
    "def repair(individual):\n",
    "    while True:\n",
    "        valid = True\n",
    "        for index in range(cluster_size):\n",
    "            while individual.usedCPUMap[index] > cluster_capacity_df.iloc[index, 1]:\n",
    "                valid = False\n",
    "                chooseTable = random.choice(individual.clusterTableMap[index])\n",
    "                newCluster = rouletteRandom(clusterRouletteProbability)\n",
    "                if newCluster != individual.entity[chooseTable]:\n",
    "                    individual.usedCPUMap[individual.entity[chooseTable]] -= table_cpu_cost_df.iloc[chooseTable, 3]\n",
    "                    individual.usedCPUMap[newCluster] += table_cpu_cost_df.iloc[chooseTable, 3]\n",
    "                    individual.clusterTableMap[individual.entity[chooseTable]].remove(chooseTable)\n",
    "                    individual.clusterTableMap[newCluster].append(chooseTable)\n",
    "                    individual.entity[chooseTable] = newCluster\n",
    "        if valid:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate a feasible individual base on parent population\n",
    "def run():\n",
    "    individual_1, individual_2 = selection(population)\n",
    "    individual = crossover(individual_1,individual_2)\n",
    "    mutate(individual)\n",
    "    repair(individual)\n",
    "    individual.figureAssessment()\n",
    "    return individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "breeding_rate = multiprocessing.cpu_count() - 1\n",
    "# one for 7 minutes\n",
    "iterations = 200\n",
    "minCrossFlows = []\n",
    "historys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform <iterations> iterations\n",
    "## generation <breeding_rate> individuals for each iteration using <breeding_rate> process to accelerate \n",
    "    \n",
    "import os\n",
    "\n",
    "for times in tqdm(range(iterations), desc='Processing'):\n",
    "    results = []\n",
    "    pool = multiprocessing.Pool(breeding_rate)\n",
    "\n",
    "    for i in range(breeding_rate):\n",
    "        results.append(pool.apply_async(func=run))\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    for result in results:\n",
    "        population.append(result.get())\n",
    "        \n",
    "    minInnerFlow = population[0].innerFlow\n",
    "    minInnerFlowIndex = 0;\n",
    "    minCrossFlow = population[0].crossFlow\n",
    "    while len(population) > population_size:\n",
    "        minInnerFlow = population[0].innerFlow\n",
    "        minInnerFlowIndex = 0;\n",
    "        for index in range(len(population)):\n",
    "            if population[index].innerFlow < minInnerFlow:\n",
    "                minInnerFlowIndex = index\n",
    "                minInnerFlow = population[index].innerFlow\n",
    "            if population[index].crossFlow < minCrossFlow:\n",
    "                minCrossFlow = population[index].crossFlow\n",
    "        del population[minInnerFlowIndex]\n",
    "    \n",
    "    minCrossFlows.append(minCrossFlow)\n",
    "    print(\"----\" + str(times + 1) + \" iteration----\")\n",
    "    print(\"minCrossFlow: \" +  str(minCrossFlow))\n",
    "    print(\"CrossFlows: \")\n",
    "    his = []\n",
    "    for i in population:\n",
    "        print(i.crossFlow)\n",
    "        his.append(i.crossFlow)\n",
    "    print(\"--------\")\n",
    "    \n",
    "    historys.append(his)\n",
    "    \n",
    "    isExists= os.path.exists(\"./placement\")\n",
    "    if not isExists:\n",
    "        os.mkdir(\"./placement\")\n",
    "    for index in range(len(population)):\n",
    "        pickle.dump(population[index], open(\"./placement/data\" + str(index), 'wb'))\n",
    "\n",
    "    pickle.dump(minCrossFlows,open(\"./placement/flows\", 'wb'))\n",
    "    pickle.dump(historys,open(\"./placement/historys\", 'wb'))\n",
    "    \n",
    "print(\"----itertions for minCrossFlow----\")\n",
    "print(minCrossFlows)\n",
    "print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = pickle.load(open(\"./placement/data\" + str(0),'rb'))\n",
    "first.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = pickle.load(open(\"./placement/flows\",'rb'))\n",
    "flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in population:\n",
    "    print(i.crossFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
